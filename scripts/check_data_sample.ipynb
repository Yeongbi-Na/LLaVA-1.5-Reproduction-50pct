{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b1b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# LLaVA 프로젝트 내부 모듈 Import\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. 샘플 데이터 (Instruction Tuning Data)\n",
    "# -------------------------------------------------------------------------\n",
    "sample_data = {\n",
    "    \"id\": \"000000334872\",\n",
    "    \"image\": \"coco/train2017/000000334872.jpg\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"<image>\\nAre the people in the image skiing downhill or cross-country skiing?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"The people in the image are cross-country skiing in the woods, as they are skiing on a trail rather than a steep slope.\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"How many people are in the image?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"There are two people in the image, both on skis in the snow.\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"What kind of environment are they skiing in?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"They are skiing in a wooded environment, following a trail through the trees while surrounded by snow.\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"Do the skiers have any additional gear with them besides their skis and poles?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"Yes, the two male skiers are carrying backpacks while they ski through the woods. The backpacks might contain essentials for their skiing adventure, such as food, water, extra clothing, or safety equipment.\"\n",
    "      }\n",
    "    ]\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704fb516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: checkpoints/llava-v1.5-7b-50pct-finetune\n",
      "Warning: 체크포인트에 토크나이저 파일이 없어서 기본 Vicuna 토크나이저를 로드합니다.\n",
      "\n",
      "================================================================================\n",
      " [Step 1] 토크나이즈 직전 (Human-Readable Prompt)\n",
      "================================================================================\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Are the people in the image skiing downhill or cross-country skiing? ASSISTANT: The people in the image are cross-country skiing in the woods, as they are skiing on a trail rather than a steep slope.</s>USER: How many people are in the image? ASSISTANT: There are two people in the image, both on skis in the snow.</s>USER: What kind of environment are they skiing in? ASSISTANT: They are skiing in a wooded environment, following a trail through the trees while surrounded by snow.</s>USER: Do the skiers have any additional gear with them besides their skis and poles? ASSISTANT: Yes, the two male skiers are carrying backpacks while they ski through the woods. The backpacks might contain essentials for their skiing adventure, such as food, water, extra clothing, or safety equipment.</s>\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      " [Step 2] 임베딩 레이어 진입 직전 (Input IDs & Labels)\n",
      "================================================================================\n",
      "Total Sequence Length: 237\n",
      "----------------------------------------\n",
      "1. Input IDs (First 60 tokens):\n",
      "[1, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, -200, 29871, 13, 17506, 278, 2305, 297, 278, 1967, 14993, 292, 1623, 29131, 470, 4891, 29899, 13509, 14993, 292, 29973, 319, 1799, 9047, 13566, 29901]\n",
      "\n",
      "   -> 이미지 토큰(-200) 존재 여부: True\n",
      "   (참고: 모델 내부에서 이 -200 자리가 576개의 Visual Embedding으로 교체됩니다.)\n",
      "----------------------------------------\n",
      "2. Labels (First 60 tokens) [-100은 Loss 계산 제외]:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "----------------------------------------\n",
      "[검증] Labels에서 -100을 제외하고 복원한 텍스트 (모델이 맞춰야 할 정답):\n",
      "\n",
      "people in the image are cross-country skiing in the woods, as they are skiing on a trail rather than a steep slope.</s> two people in the image, both on skis in the snow.</s>USERing in a wooded environment, following a trail through the trees while surrounded by snow.</s>USER: male skiers are carrying backpacks while they ski through the woods. The backpacks might contain essentials for their skiing adventure, such as food, water, extra clothing, or safety equipment.</s>\n",
      "----------------------------------------\n",
      "-> 위 텍스트에 '질문'은 없고 '답변'과 '</s>'만 보이면 정상입니다.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. 모델 및 토크나이저 경로 설정\n",
    "# ---------------------------------------------------------------------\n",
    "# 학습된 체크포인트 경로 사용\n",
    "model_path = \"checkpoints/llava-v1.5-7b-50pct-finetune\"\n",
    "\n",
    "print(f\"Loading tokenizer from: {model_path}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        model_max_length=2048,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False\n",
    "    )\n",
    "except OSError:\n",
    "    print(\"Warning: 체크포인트에 토크나이저 파일이 없어서 기본 Vicuna 토크나이저를 로드합니다.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\", use_fast=False)\n",
    "\n",
    "# LLaVA 특수 토큰 처리 (이미지 토큰 등)\n",
    "tokenizer.add_tokens([DEFAULT_IMAGE_TOKEN], special_tokens=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# [Step 1] 토크나이즈 직전: 프롬프트 구성 (Prompt Construction)\n",
    "# ---------------------------------------------------------------------\n",
    "# LLaVA-1.5는 'vicuna_v1' 템플릿 사용\n",
    "conv = conv_templates[\"vicuna_v1\"].copy()\n",
    "roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "# 대화 내용 구성\n",
    "source = sample_data[\"conversations\"]\n",
    "\n",
    "# 첫 대화가 Human이 아니면 스킵 (LLaVA train.py 로직)\n",
    "if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "    source = source[1:]\n",
    "\n",
    "conv.messages = []\n",
    "for j, sentence in enumerate(source):\n",
    "    role = roles[sentence[\"from\"]]\n",
    "    conv.append_message(role, sentence[\"value\"])\n",
    "\n",
    "# 최종 프롬프트 문자열 생성\n",
    "full_prompt = conv.get_prompt()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" [Step 1] 토크나이즈 직전 (Human-Readable Prompt)\")\n",
    "print(\"=\"*80)\n",
    "print(full_prompt)\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# [Step 2] 임베딩 직전: Input IDs & Labels (Masking 적용)\n",
    "# ---------------------------------------------------------------------\n",
    "# LLaVA의 train.py -> preprocess_v1() 함수 로직을 단순화하여 구현\n",
    "\n",
    "# 1. Input IDs 생성 \n",
    "#    (이미지 텍스트 <image>는 IMAGE_TOKEN_INDEX (-200)로 변환됨)\n",
    "input_ids = tokenizer_image_token(full_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "\n",
    "# 2. Labels 생성 (Input IDs 복제 후 User 부분 마스킹)\n",
    "targets = input_ids.clone()\n",
    "\n",
    "# 3. Masking 로직 (User 발화 부분 -100 처리)\n",
    "sep = conv.sep + conv.roles[1] + \": \" # \" ASSISTANT: \"\n",
    "\n",
    "# 전체 대화를 턴 단위로 분리 (Vicuna는 </s>로 구분됨)\n",
    "rounds = full_prompt.split(conv.sep2) \n",
    "\n",
    "cur_len = 1 # <s> (BOS) 토큰 길이\n",
    "targets[:cur_len] = IGNORE_INDEX # 시작 토큰 마스킹\n",
    "\n",
    "for i, rou in enumerate(rounds):\n",
    "    if rou == \"\": break\n",
    "\n",
    "    parts = rou.split(sep)\n",
    "    if len(parts) != 2: break\n",
    "    \n",
    "    # parts[0]: \"USER: <image>\\nQuestion\"\n",
    "    # parts[1]: \"Answer\"\n",
    "    \n",
    "    # 구분자 복구 (\"USER: ... ASSISTANT: \" 까지가 Instruction)\n",
    "    parts[0] += sep \n",
    "\n",
    "    # Instruction 길이 계산 (토크나이징해서 길이 측정)\n",
    "    instruction_ids = tokenizer_image_token(parts[0], tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "    \n",
    "    # <s> 토큰(1개) 제외하고 길이 계산\n",
    "    instruction_len = len(instruction_ids) - 1 \n",
    "\n",
    "    # [마스킹 핵심] \n",
    "    # 현재 위치(cur_len) 부터 Instruction이 끝나는 곳까지 -100으로 덮어씀\n",
    "    targets[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "    # 다음 라운드 시작 위치 계산\n",
    "    round_ids = tokenizer_image_token(rou, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "    cur_len += len(round_ids) - 1 # <s> 제외\n",
    "    \n",
    "    # EOS (</s>) 토큰 처리\n",
    "    cur_len += 1 \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" [Step 2] 임베딩 레이어 진입 직전 (Input IDs & Labels)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Total Sequence Length: {len(input_ids)}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 앞부분 60개 토큰만 출력해서 확인\n",
    "print(f\"1. Input IDs (First 60 tokens):\\n{input_ids[:60].tolist()}\")\n",
    "\n",
    "# -200 확인\n",
    "has_image = IMAGE_TOKEN_INDEX in input_ids\n",
    "print(f\"\\n   -> 이미지 토큰(-200) 존재 여부: {has_image}\")\n",
    "if has_image:\n",
    "    print(\"   (참고: 모델 내부에서 이 -200 자리가 576개의 Visual Embedding으로 교체됩니다.)\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"2. Labels (First 60 tokens) [-100은 Loss 계산 제외]:\\n{targets[:60].tolist()}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 검증: 실제 모델이 학습하는 내용 복원\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"-\" * 40)\n",
    "print(\"[검증] Labels에서 -100을 제외하고 복원한 텍스트 (모델이 맞춰야 할 정답):\")\n",
    "\n",
    "valid_tokens = targets[targets != IGNORE_INDEX]\n",
    "decoded_truth = tokenizer.decode(valid_tokens, skip_special_tokens=False)\n",
    "\n",
    "print(\"\\n\" + decoded_truth)\n",
    "print(\"-\" * 40)\n",
    "print(\"-> 위 텍스트에 '질문'은 없고 '답변'과 '</s>'만 보이면 정상입니다.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64360fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Are the people in the image skiing downhill or cross-country skiing? ASSISTANT: The people in the image are cross-country skiing in the woods, as they are skiing on a trail rather than a steep slope.</s>USER: How many people are in the image? ASSISTANT: There are two people in the image, both on skis in the snow.</s>USER: What kind of environment are they skiing in? ASSISTANT: They are skiing in a wooded environment, following a trail through the trees while surrounded by snow.</s>USER: Do the skiers have any additional gear with them besides their skis and poles? ASSISTANT: Yes, the two male skiers are carrying backpacks while they ski through the woods. The backpacks might contain essentials for their skiing adventure, such as food, water, extra clothing, or safety equipment.</s>\n"
     ]
    }
   ],
   "source": [
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecdf1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dbae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d53f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LLaVA 전처리 과정 확인 스크립트\n",
    "1. 토크나이즈 직전 텍스트 형태\n",
    "2. 임베딩 레이어 직전 input_ids 형태\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# LLaVA 경로 추가\n",
    "sys.path.insert(0, '/nas/home/ongv1109/LLaVA1.5')\n",
    "\n",
    "from llava.model import LlavaLlamaForCausalLM\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ================== 설정 ==================\n",
    "MODEL_PATH = \"/nas/home/ongv1109/LLaVA1.5/checkpoints/llava-v1.5-7b-50pct-finetune\"\n",
    "IMAGE_FOLDER = \"/nas/datahub/llava-v1.5-instruct\"\n",
    "\n",
    "# 샘플 데이터\n",
    "SAMPLE = {\n",
    "    \"id\": \"000000334872\",\n",
    "    \"image\": \"coco/train2017/000000334872.jpg\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"<image>\\nAre the people in the image skiing downhill or cross-country skiing?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"The people in the image are cross-country skiing in the woods, as they are skiing on a trail rather than a steep slope.\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"How many people are in the image?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"There are two people in the image, both on skis in the snow.\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"What kind of environment are they skiing in?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"They are skiing in a wooded environment, following a trail through the trees while surrounded by snow.\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"Do the skiers have any additional gear with them besides their skis and poles?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"Yes, the two male skiers are carrying backpacks while they ski through the woods. The backpacks might contain essentials for their skiing adventure, such as food, water, extra clothing, or safety equipment.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"섹션 구분선 출력\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"모델과 토크나이저 로드\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        model_max_length=2048,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    \n",
    "    # 모델 로드 (CPU로, 추론용)\n",
    "    model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def preprocess_multimodal(conversations):\n",
    "    \"\"\"\n",
    "    LLaVA의 멀티모달 전처리\n",
    "    <image> 토큰을 DEFAULT_IMAGE_TOKEN으로 교체\n",
    "    \"\"\"\n",
    "    for sentence in conversations:\n",
    "        if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
    "            sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "            sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\n",
    "            sentence['value'] = sentence['value'].strip()\n",
    "    return conversations\n",
    "\n",
    "def preprocess_llama2(conversations, tokenizer):\n",
    "    \"\"\"\n",
    "    LLaMA-2 스타일 대화 형식으로 변환\n",
    "    [INST] ... [/INST] 형식\n",
    "    \"\"\"\n",
    "    conv = conv_templates[\"vicuna_v1\"].copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "    \n",
    "    # 대화 추가\n",
    "    for j, sentence in enumerate(conversations):\n",
    "        role = roles[sentence[\"from\"]]\n",
    "        conv.append_message(role, sentence[\"value\"])\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def main():\n",
    "    print_section(\"LLaVA 전처리 과정 확인\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    \n",
    "    # ================== STEP 1: 원본 대화 ==================\n",
    "    print_section(\"STEP 1: 원본 대화 (JSON)\")\n",
    "    print(json.dumps(SAMPLE['conversations'], indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # ================== STEP 2: 멀티모달 전처리 ==================\n",
    "    print_section(\"STEP 2: 멀티모달 전처리 후\")\n",
    "    conversations = preprocess_multimodal(SAMPLE['conversations'].copy())\n",
    "    \n",
    "    for i, conv in enumerate(conversations):\n",
    "        print(f\"[{i}] {conv['from']}: {repr(conv['value'][:100])}...\")\n",
    "    \n",
    "    # ================== STEP 3: 대화 템플릿 적용 (토크나이즈 직전) ==================\n",
    "    print_section(\"STEP 3: 토크나이즈 직전 텍스트\")\n",
    "    \n",
    "    prompt = preprocess_llama2(conversations, tokenizer)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"전체 프롬프트:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"길이: {len(prompt)} 문자\")\n",
    "    print(f\"<image> 토큰 위치: {prompt.find(DEFAULT_IMAGE_TOKEN)}\")\n",
    "    \n",
    "    # ================== STEP 4: 토크나이즈 후 (임베딩 직전) ==================\n",
    "    print_section(\"STEP 4: 토크나이즈 후 input_ids (임베딩 레이어 직전)\")\n",
    "    \n",
    "    # 토크나이즈\n",
    "    input_ids = tokenizer_image_token(\n",
    "        prompt, \n",
    "        tokenizer, \n",
    "        IMAGE_TOKEN_INDEX, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    print(f\"input_ids shape: {input_ids.shape}\")\n",
    "    print(f\"input_ids dtype: {input_ids.dtype}\")\n",
    "    print(f\"\\ninput_ids (처음 50개):\")\n",
    "    print(input_ids[:50])\n",
    "    print(f\"\\ninput_ids (전체):\")\n",
    "    print(input_ids)\n",
    "    \n",
    "    # IMAGE_TOKEN_INDEX 위치 찾기\n",
    "    image_token_positions = (input_ids == IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[0]\n",
    "    print(f\"\\nIMAGE_TOKEN_INDEX ({IMAGE_TOKEN_INDEX}) 위치:\")\n",
    "    print(image_token_positions)\n",
    "    \n",
    "    # ================== STEP 5: 토큰 ID → 텍스트 매핑 ==================\n",
    "    print_section(\"STEP 5: 토큰 ID와 텍스트 매핑 (샘플)\")\n",
    "    \n",
    "    print(\"처음 20개 토큰:\")\n",
    "    for i in range(min(20, len(input_ids))):\n",
    "        token_id = input_ids[i].item()\n",
    "        if token_id == IMAGE_TOKEN_INDEX:\n",
    "            token_text = \"<IMAGE_TOKEN>\"\n",
    "        else:\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "        print(f\"  [{i:3d}] ID={token_id:6d} → {repr(token_text)}\")\n",
    "    \n",
    "    # ================== STEP 6: 이미지 전처리 ==================\n",
    "    print_section(\"STEP 6: 이미지 전처리\")\n",
    "    \n",
    "    image_path = Path(IMAGE_FOLDER) / SAMPLE['image']\n",
    "    if image_path.exists():\n",
    "        print(f\"이미지 경로: {image_path}\")\n",
    "        \n",
    "        # 이미지 로드\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        print(f\"원본 이미지 크기: {image.size}\")\n",
    "        \n",
    "        # CLIP 전처리 (LLaVA의 process_images 사용)\n",
    "        from llava.mm_utils import get_model_name_from_path\n",
    "        from llava.model.builder import load_pretrained_model\n",
    "        \n",
    "        # Vision tower의 image processor 가져오기\n",
    "        image_processor = model.get_vision_tower().image_processor\n",
    "        \n",
    "        # 이미지 전처리\n",
    "        image_tensor = process_images([image], image_processor, model.config)\n",
    "        \n",
    "        print(f\"전처리 후 이미지 텐서 shape: {image_tensor.shape}\")\n",
    "        print(f\"전처리 후 이미지 텐서 dtype: {image_tensor.dtype}\")\n",
    "        print(f\"전처리 후 이미지 텐서 범위: [{image_tensor.min():.3f}, {image_tensor.max():.3f}]\")\n",
    "    else:\n",
    "        print(f\"⚠️  이미지를 찾을 수 없습니다: {image_path}\")\n",
    "    \n",
    "    # ================== STEP 7: 요약 ==================\n",
    "    print_section(\"요약\")\n",
    "    \n",
    "    print(f\"✓ 원본 대화 턴 수: {len(SAMPLE['conversations'])}\")\n",
    "    print(f\"✓ 토크나이즈 전 텍스트 길이: {len(prompt)} 문자\")\n",
    "    print(f\"✓ 토크나이즈 후 시퀀스 길이: {len(input_ids)} 토큰\")\n",
    "    print(f\"✓ IMAGE_TOKEN 개수: {len(image_token_positions)}\")\n",
    "    print(f\"✓ IMAGE_TOKEN_INDEX 값: {IMAGE_TOKEN_INDEX}\")\n",
    "    \n",
    "    if image_path.exists():\n",
    "        print(f\"✓ 이미지 전처리 완료: {image_tensor.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"전처리 과정 확인 완료!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
